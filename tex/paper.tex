\documentclass[a4paper]{tufte-handout}

\title{Using Automatic Feedback Control to Automatically Tune Machine Learning Models}

\author[H. Stein Shiromoto]{Humberto Stein Shiromoto}

%\date{28 March 2010} % without \date command, current date is supplied

%\geometry{showframe} % display margins for debugging page layout

\usepackage{graphicx} % allow embedded images
  \setkeys{Gin}{width=\linewidth,totalheight=\textheight,keepaspectratio}
  \graphicspath{{graphics/}} % set of paths to search for images
\usepackage{amsmath}  % extended mathematics
\usepackage{booktabs} % book-quality tables
\usepackage{units}    % non-stacked fractions and better unit spacing
\usepackage{multicol} % multiple column layout facilities
\usepackage{fancyvrb} % extended verbatim environments
  \fvset{fontsize=\normalsize}% default font size for fancy-verbatim environments

% Standardize command font styles and environments
\newcommand{\doccmd}[1]{\texttt{\textbackslash#1}}% command name -- adds backslash automatically
\newcommand{\docopt}[1]{\ensuremath{\langle}\textrm{\textit{#1}}\ensuremath{\rangle}}% optional command argument
\newcommand{\docarg}[1]{\textrm{\textit{#1}}}% (required) command argument
\newcommand{\docenv}[1]{\textsf{#1}}% environment name
\newcommand{\docpkg}[1]{\texttt{#1}}% package name
\newcommand{\doccls}[1]{\texttt{#1}}% document class name
\newcommand{\docclsopt}[1]{\texttt{#1}}% document class option name
\newenvironment{docspec}{\begin{quote}\noindent}{\end{quote}}% command specification environment

\begin{document}

\maketitle% this prints the handout title, author, and date

\begin{abstract}
\noindent
This paper uses feedback-control approaches to tune machine learning models.
\end{abstract}

%\printclassoptions

\section{Introduction}

The essential problem within supervised learning is to solve the problem
\begin{eqnarray}
  f=\arg\min_\theta&\mathcal{L}(y,\hat{y},\theta)\;,
\end{eqnarray}
where $\mathcal{L}:\mathbb{R}^n\times\mathbb{R}^n\times\mathbb{R}^m\to\mathbb{R}_{\geq0}$ is a differentiable functional\sidenote{Needs definition} called \emph{loss}, $y\in\mathbb{R}^n$ is the \emph{observation} vector, $\hat{y}\in\mathbb{R}^n$ is the \emph{prediction} vector, and, for every time $t\in\mathbb{Z}$, the \emph{hyperparameter} $\theta$ evolves in $\mathbb{R}^m$.

Now assume that the time-evolution of $\theta$ is, in accordance to the difference equation
\begin{equation}
\theta^+=g(y,\hat{y})+h(y,\hat{y})\theta
\end{equation}

What is tried to be proven here is the following:

If there exists a pair\sidenote{Needs definition} $(V,\theta)$ consisting of a Lyapunov function $V$ and hyperparameters $\theta$ such that the inequality
$$L_{g+f}V(y,\hat{y},\theta)\leq -V(y,\hat{y})$$
holds for all $y$ and $\hat{y}$, then $(V,\theta)$ is optimal. 

\end{document}